{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seismic Transformer V4.0 Cook book\n",
    "\n",
    "*Author: Jason Jiang (Xunfun Lee)*\n",
    "\n",
    "*Date: 2024.02.01*\n",
    "\n",
    "Adding structural embedding based on SeT-3, while all the other parts are the same as SeT-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: cuda\n",
      "CUDA device numbers:  1\n"
     ]
    }
   ],
   "source": [
    "from PythonScripts.utility import SetDevice\n",
    "\n",
    "device = SetDevice()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structural Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class StructuralInfoEmbedding(nn.Module):\n",
    "    \"\"\"putting structural info into the embedding, including IM type, struct type, stories and height.\n",
    "    \n",
    "    Args:\n",
    "        IM_type (int): IM = index 0, 1, 2 for 6, 7, 8 magnitude\n",
    "        sturct_type (int): 0:RC-Frame, 1:RC-Shearwall\n",
    "        type_embedding_size (int): type embedding size\n",
    "        middle_size (int): first fc size\n",
    "        hidden_size (int): output size, the same as the SeT hidden size\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 IM_type:int=3,                     # IM = 6, 7, 8\n",
    "                 sturct_type:int=2,                 # 0:RC-Frame, 1:RC-Shearwall\n",
    "                 type_embedding_size:int=5,         # type embedding size\n",
    "                 middle_size:int=128,               # middle size\n",
    "                 hidden_size:int=768):              # hidden size\n",
    "\n",
    "        super(StructuralInfoEmbedding, self).__init__()\n",
    "\n",
    "        # type embedding (IM type and struct type)\n",
    "        self.IM_type_embedding = nn.Embedding(IM_type, type_embedding_size)\n",
    "        self.struct_type_embedding = nn.Embedding(sturct_type, type_embedding_size)\n",
    "\n",
    "        # number of numeric feature (stories and height)\n",
    "        self.numeric_feature_size = 2\n",
    "\n",
    "        # combine type embedding and numeric feature\n",
    "        self.expansion_fc = nn.Sequential(\n",
    "            nn.Linear(self.numeric_feature_size + type_embedding_size * 2, middle_size),            # (2 + 5*2)=12 -> 128\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(middle_size, hidden_size)                                                     # 128 -> 768\n",
    "        )\n",
    "\n",
    "    def forward(self, structural_info):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            structural_info : dict\n",
    "                stories: torch.Tensor, dtype=torch.float\n",
    "                height: torch.Tensor, dtype=torch.float\n",
    "                IM_type: torch.Tensor, dtype=torch.long (64-bit integer)\n",
    "                struct_type: torch.Tensor, dtype=torch.long (64-bit integer)\n",
    "        \"\"\"\n",
    "        # type embedding\n",
    "        IM_embed = self.IM_type_embedding(structural_info[\"IM_type\"])\n",
    "        struct_type_embed = self.struct_type_embedding(structural_info[\"struct_type\"])\n",
    "\n",
    "        # standardize numeric features\n",
    "        stories_normalized = (structural_info[\"stories\"] - 5.5) / 2.9  # stories in 1-10F\n",
    "        height_normalized = (structural_info[\"height\"] - 15.5) / 8.7  # height in 3-30m\n",
    "        numeric_features = torch.stack([stories_normalized, height_normalized], dim=1)\n",
    "\n",
    "        # combine type embedding and numeric feature\n",
    "        structuralInfo_features = torch.cat([IM_embed, struct_type_embed, numeric_features], dim=1)\n",
    "\n",
    "        # using fc to expand structural info features to 768                                        # 12 -> 768\n",
    "        structural_info_embedding = self.expansion_fc(structuralInfo_features)\n",
    "\n",
    "        return structural_info_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIE_instance = StructuralInfoEmbedding().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.], device='cuda:0'),\n",
       " tensor([9.], device='cuda:0'),\n",
       " tensor([1], device='cuda:0'),\n",
       " tensor([0], device='cuda:0'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 假设batch_size为1，如果想要测试更多样本可以增加batch_size\n",
    "batch_size = 1\n",
    "\n",
    "# 初始化模型\n",
    "structural_info_embedding_model = StructuralInfoEmbedding().to(device)\n",
    "\n",
    "# 创建测试样本的特征\n",
    "# 抗震设防烈度映射：6度->0, 7度->1, 8度->2\n",
    "# 结构类型映射：框架结构->0, 框架剪力墙结构->1\n",
    "stories_test = torch.tensor([3], dtype=torch.float).to(device)  # 层数\n",
    "height_test = torch.tensor([9], dtype=torch.float).to(device)   # 高度\n",
    "IM_type_test = torch.tensor([1]).to(device)                    # 7度抗震设防烈度\n",
    "struct_type_test = torch.tensor([0]).to(device)                # 框架结构\n",
    "\n",
    "# 使用元组来存储结构数据\n",
    "structural_info_dict = {\"stories\": stories_test, \n",
    "                        \"height\": height_test, \n",
    "                        \"IM_type\": IM_type_test, \n",
    "                        \"struct_type\": struct_type_test}\n",
    "\n",
    "\n",
    "stories_test, height_test, IM_type_test, struct_type_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.], device='cuda:0'),\n",
       " tensor([9.], device='cuda:0'),\n",
       " tensor([1], device='cuda:0'),\n",
       " tensor([0], device='cuda:0'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structural_info_dict[\"stories\"], structural_info_dict[\"height\"], structural_info_dict[\"IM_type\"], structural_info_dict[\"struct_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取结构信息嵌入\n",
    "structural_info_embedding_test = structural_info_embedding_model(structural_info_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structural_info_embedding_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the SeT-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EncoderV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PythonScripts.transformer import PatchEmbeddingBlock, FreqEmbeddingBlock, EncoderBlock\n",
    "\n",
    "class EncoderV2(nn.Module):\n",
    "    \"\"\"EncoderV2 combined encoder block (MLP + MHA), PE, SE and SIE, block\n",
    "\n",
    "    (batch_size, 3000, 1) --> (batch_size, 14, 768)\n",
    "    \n",
    "    Args:\n",
    "        len_gm (int): Length of the ground motion. Defaults to 3000.\n",
    "        patch_size (int): Size of the patch. Defaults to 250.\n",
    "        hidden_size (int): Hidden size of the input tensor. Defaults to 768.\n",
    "        num_heads (int): Number of attention heads. Defaults to 12.\n",
    "        fc_hidden_size (int): Hidden size of the first fully connected layer. Defaults to 3072.\n",
    "        dropout_attn (float): Dropout rate. Defaults to 0.1.\n",
    "        dropout_mlp (float): Dropout rate. Defaults to 0.1.\n",
    "        dropout_embed (float): Dropout rate. Defaults to 0.1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 len_gm:int=3000,\n",
    "                 patch_size:int=250,\n",
    "                 hidden_size:int=768,\n",
    "                 num_heads:int=12,\n",
    "                 num_layers:int=12,\n",
    "                 dropout_attn:float=0.1,\n",
    "                 dropout_mlp:float=0.1,\n",
    "                 dropout_embed:float=0.1):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Calculate the number of patches\n",
    "        self.num_of_patch = len_gm // patch_size\n",
    "\n",
    "        # Initialize a variable to stroe the attention weights\n",
    "        self.attention_weights_list = []  # Initialize it here\n",
    "        \n",
    "        # BLOCK\n",
    "        # patch embedding\n",
    "        self.PatchEmbedding = PatchEmbeddingBlock(len_gm=len_gm,\n",
    "                                      patch_size=patch_size,\n",
    "                                      output_size=hidden_size)\n",
    "        \n",
    "        # frequency embedding\n",
    "        self.FreqEmbedding = FreqEmbeddingBlock(conv_output_size=len_gm // 2 // 2,         # default is 750\n",
    "                                     linear_output_size=hidden_size)\n",
    "\n",
    "        # structural info embedding\n",
    "        self.StruEmbedding = StructuralInfoEmbedding(IM_type=3,                             # IM = 6, 7, 8\n",
    "                                                     sturct_type=2,                         # 0:RC-Frame, 1:RC-Shearwall\n",
    "                                                     type_embedding_size=5,                 # type embedding size\n",
    "                                                     middle_size=128,                       # first fc size\n",
    "                                                     hidden_size=hidden_size)\n",
    "\n",
    "        # encoder layer\n",
    "        self.EncoderLayers = nn.Sequential(*[EncoderBlock(hidden_size=hidden_size,\n",
    "                                                          num_heads=num_heads,\n",
    "                                                          fc_hidden_size=hidden_size*4,\n",
    "                                                          dropout_attn=dropout_attn,\n",
    "                                                          dropout_mlp=dropout_mlp) for _ in range(num_layers)])\n",
    "\n",
    "        # [TOKEN]\n",
    "        # [TIME] - time token\n",
    "        self.time_token = nn.Parameter(torch.randn(1, 1, hidden_size),\n",
    "                                       requires_grad=True)  # trainable parameter\n",
    "\n",
    "        # [FREQ] - frequency token\n",
    "        self.freq_token = nn.Parameter(torch.randn(1, 1, hidden_size),\n",
    "                                       requires_grad=True)  # trainable parameter\n",
    "        \n",
    "        # [STRU] - structural info token\n",
    "        self.stru_token = nn.Parameter(torch.randn(1, 1, hidden_size),\n",
    "                                                  requires_grad=True)\n",
    "\n",
    "        # [CLS] - class token\n",
    "        self.class_token = nn.Parameter(torch.randn(1, 1, hidden_size),\n",
    "                                        requires_grad=True)  # trainable parameter\n",
    "\n",
    "        # POSITION\n",
    "        # positional embedding\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(1, self.num_of_patch+2, hidden_size),\n",
    "                                                  requires_grad=True)  # trainable parameter\n",
    "        \n",
    "        # Dropout\n",
    "        self.embedding_dropout = nn.Dropout(dropout_embed)\n",
    "\n",
    "\n",
    "    def forward(self, ground_motion, structural_info, key_padding_mask=None, need_weights=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ground_motion: (batch_size, 3000, 1) - dtype=torch.float\n",
    "            structural_info: (stories, height, IM_type, struct_type)\n",
    "                            - dict[torch.Tensor-dtype=float, torch.Tensor-dtype=float, torch.Tensor-dtype=long, torch.Tensor-dtype=long]\n",
    "            key_padding_mask: (batch_size, 14) - dtype=torch.bool\n",
    "            need_weights: bool\n",
    "        \"\"\"\n",
    "        # Get the batch size\n",
    "        batch_size = ground_motion.shape[0]\n",
    "\n",
    "        # Structural info embedding                                                                         \n",
    "        structural_info_embedding = self.StruEmbedding(structural_info)                             # [batch_size, 1, hidden_size]\n",
    "\n",
    "        # [STRU] token\n",
    "        stru_tokens = self.stru_token.repeat(batch_size, 1, 1)\n",
    "\n",
    "        # concatenate the structural info embedding with the structural info tokens\n",
    "        stru_sequence_with_token = structural_info_embedding + stru_tokens                          # [batch_size, 1, hidden_size]\n",
    "\n",
    "        # clear the attention weights list\n",
    "        self.attention_weights_list = []\n",
    "\n",
    "        # patch embedding\n",
    "        time_sequence = self.PatchEmbedding(ground_motion)\n",
    "\n",
    "        # [TIME] token\n",
    "        time_tokens = self.time_token.repeat(batch_size, self.num_of_patch, 1)\n",
    "\n",
    "        # concatenate the time sequence with the time tokens\n",
    "        time_sequence_with_token = time_sequence + time_tokens                                      # [batch_size, 12, hidden_size]\n",
    "\n",
    "        # frequency embedding\n",
    "        freq_sequence = self.FreqEmbedding(ground_motion)\n",
    "\n",
    "        # [FREQ] token\n",
    "        freq_tokens = self.freq_token.repeat(batch_size, 1, 1)\n",
    "\n",
    "        # concatenate the frequency sequence with the frequency tokens\n",
    "        freq_sequence_with_token = freq_sequence + freq_tokens                                      # [batch_size, 1, hidden_size]\n",
    "\n",
    "        # cat the stru sequence, time sequence and the frequency sequence\n",
    "        sequence_combine = torch.cat((stru_sequence_with_token, time_sequence_with_token, freq_sequence_with_token), dim=1)   # [batch_size, 14, hidden_size]\n",
    "\n",
    "        # [CLS] token\n",
    "        class_tokens = self.class_token.expand(batch_size, -1, -1) # \"-1\" means to infer the dimension (try this line on its own)\n",
    "\n",
    "        # concatenate the class token with the sequence\n",
    "        sequence_combine_with_cls = torch.cat((class_tokens, sequence_combine), dim=1)              # [batch_size, 15, hidden_size]\n",
    "\n",
    "        # embedding dropout\n",
    "        x = self.embedding_dropout(sequence_combine_with_cls)\n",
    "\n",
    "        # Encoder Layer\n",
    "        for layer in self.EncoderLayers:\n",
    "            x, attn_weights = layer(x, key_padding_mask=key_padding_mask, need_weights=need_weights)\n",
    "            self.attention_weights_list.append(attn_weights)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    # test code\n",
    "    '''python\n",
    "    EncoderV2_Instance = EncoderV2().to(device)\n",
    "    input_Encoder = torch.rand(64, 3000, 1).to(device)\n",
    "    output = EncoderV2_Instance(input_Encoder)\n",
    "\n",
    "    # output.shape = torch.Size([64, 15, 768])\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 15, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EncoderV2_Instance = EncoderV2().to(device)\n",
    "input_Encoder = torch.rand(64, 3000, 1).to(device)\n",
    "output = EncoderV2_Instance(input_Encoder, structural_info_dict)\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SeismicTransformer V4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PythonScripts.transformer import DecoderV1, ClassifierV1, SplicerV1\n",
    "\n",
    "class SeismicTransformerV4(nn.Module):\n",
    "    \"\"\"Seismic Transformer V4.0 class, including encoder, decoder, classifier and splicer.\n",
    "    \n",
    "    Args:\n",
    "        len_gm (int): Length of the ground motion. Defaults to 3000.\n",
    "        patch_size (int): Size of the patch. Defaults to 250.\n",
    "        hidden_size (int): Hidden size of the input tensor. Defaults to 768.\n",
    "        num_heads (int): Number of attention heads. Defaults to 12.\n",
    "        num_layers (int): Number of layers. Defaults to 12.\n",
    "        dropout_attn (float): Dropout rate. Defaults to 0.1.\n",
    "        dropout_mlp (float): Dropout rate. Defaults to 0.1.\n",
    "        dropout_embed (float): Dropout rate. Defaults to 0.1.\n",
    "        num_of_classes (int): Number of classes. Defaults to 5.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 len_gm:int=3000,\n",
    "                 patch_size:int=250,\n",
    "                 hidden_size:int=768,\n",
    "                 num_heads:int=12,\n",
    "                 num_layers:int=12,\n",
    "                 dropout_attn:float=0.1,\n",
    "                 dropout_mlp:float=0.1,\n",
    "                 dropout_embed:float=0.1,\n",
    "                 num_of_classes:int=5):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = EncoderV2(len_gm=len_gm,\n",
    "                                 patch_size=patch_size,\n",
    "                                 hidden_size=hidden_size,\n",
    "                                 num_heads=num_heads,\n",
    "                                 num_layers=num_layers,\n",
    "                                 dropout_attn=dropout_attn,\n",
    "                                 dropout_mlp=dropout_mlp,\n",
    "                                 dropout_embed=dropout_embed)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = DecoderV1(len_gm=len_gm,\n",
    "                                 patch_size=patch_size,\n",
    "                                 hidden_size=hidden_size,\n",
    "                                 num_heads=num_heads,\n",
    "                                 num_layers=num_layers,\n",
    "                                 dropout_attn=dropout_attn,\n",
    "                                 dropout_mlp=dropout_mlp,\n",
    "                                 dropout_embed=dropout_embed)\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = ClassifierV1(hidden_size=hidden_size,\n",
    "                                       num_of_classes=num_of_classes)\n",
    "        \n",
    "        # Splicer\n",
    "        self.splicer = SplicerV1(hidden_size=hidden_size,\n",
    "                                 patch_size=patch_size,\n",
    "                                 len_gm=len_gm)\n",
    "        \n",
    "    def forward(self, encoder_input, struct_info, decoder_input=None, key_padding_mask=None, attn_mask=None, teacher_forcing_ratio=1.0):\n",
    "        # Encoder output\n",
    "        encoder_output = self.encoder(ground_motion=encoder_input, structural_info=struct_info ,key_padding_mask=key_padding_mask)\n",
    "\n",
    "        encoder_output_to_decoder = encoder_output[:,2:14,:]            # for adding structural info embedding in front of the time sequence, so change from [:,1:13,:] to [:,2:14,:]\n",
    "\n",
    "        # If target sequence is provided, we are in training mode, otherwise we are in inference mode\n",
    "        decoder_output = self.decoder(output_encoder=encoder_output_to_decoder,\n",
    "                                        decoder_input=decoder_input,\n",
    "                                        attn_mask=attn_mask,\n",
    "                                        need_weights=True,\n",
    "                                        teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "            \n",
    "        dynamic_response = self.splicer(decoder_output)\n",
    "\n",
    "        # Classifier forward pass to determine the damage state\n",
    "        # damage_state is logits, put 0 index logit through classifier\n",
    "        damage_state = self.classifier(encoder_output[:, 0])\n",
    "\n",
    "        return damage_state, dynamic_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 5]) torch.Size([64, 3000, 1])\n",
      "torch.Size([64, 5]) torch.Size([64, 3000, 1])\n"
     ]
    }
   ],
   "source": [
    "SeismicTransformerV4_instance = SeismicTransformerV4().to(device)\n",
    "input_gm = torch.rand(64, 3000, 1).to(device)\n",
    "input_floorResponse = torch.rand(64, 3000, 1).to(device)\n",
    "struct_info = {\"stories\": torch.tensor([3], dtype=torch.float).to(device),\n",
    "                \"height\": torch.tensor([9], dtype=torch.float).to(device),\n",
    "                \"IM_type\": torch.tensor([1]).to(device),\n",
    "                \"struct_type\": torch.tensor([0]).to(device)}\n",
    "\n",
    "# key_padding_mask must be match the sequence = 15\n",
    "key_padding_mask = torch.zeros(64, 15, dtype=torch.bool).to(device)\n",
    "attn_mask = torch.triu(torch.ones(12, 12), diagonal=1).bool().to(device)\n",
    "\n",
    "# training mode (with encoder input)\n",
    "damage_state, dynamic_response = SeismicTransformerV4_instance(encoder_input=input_gm, \n",
    "                                                               struct_info=struct_info,\n",
    "                                                                decoder_input=input_floorResponse, \n",
    "                                                                key_padding_mask=key_padding_mask, \n",
    "                                                                attn_mask=attn_mask)\n",
    "\n",
    "print(damage_state.shape, dynamic_response.shape)\n",
    "# output - (torch.Size([64, 5]), torch.Size([64, 3000, 1]))\n",
    "\n",
    "# inference mode (without encoder input)\n",
    "with torch.inference_mode():\n",
    "    damage_state, dynamic_response = SeismicTransformerV4_instance(encoder_input=input_gm, \n",
    "                                                                    struct_info=struct_info,\n",
    "                                                                    decoder_input=None, \n",
    "                                                                    key_padding_mask=key_padding_mask, \n",
    "                                                                    attn_mask=attn_mask)\n",
    "print(damage_state.shape, dynamic_response.shape)\n",
    "# output - (torch.Size([64, 5]), torch.Size([64, 3000, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
