{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seismic Transformer V4.0 train book\n",
    "\n",
    "*Author: Jason Jiang (Xunfun Lee)*\n",
    "\n",
    "*Date: 2024.02.01*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: cuda\n",
      "CUDA device numbers:  1\n"
     ]
    }
   ],
   "source": [
    "from PythonScripts.utility import SetDevice\n",
    "\n",
    "device = SetDevice()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "From SeT-4 we are using totally different data and data format. The data is stored in `h5` format, which is a binary format. And we have 132GB data in total, which is too large to be loaded into memory. So we need to load the data in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PythonScripts.data_preparation import DynamicDatasetV1\n",
    "import torch\n",
    "\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the dataset\n",
    "gm_file_path = 'D:/SesimicTransformerData/All_GMs/GMs_knet_3474_AF_57.h5'\n",
    "building_files_dir = 'D:/SeismicTransformerData/SeT-4.0'\n",
    "\n",
    "dataset = DynamicDatasetV1(gm_file_path=gm_file_path, \n",
    "                           building_files_dir=building_files_dir, \n",
    "                           device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    gm_data_list, building_attributes_list, acc_floor_response_list, blg_damage_state_list = zip(*batch)\n",
    "\n",
    "    # Stack ground motion data, floor response data, and damage state data\n",
    "    gm_data_batch = torch.stack(gm_data_list)\n",
    "    acc_floor_response_batch = torch.stack(acc_floor_response_list)\n",
    "    blg_damage_state_batch = torch.stack(blg_damage_state_list)\n",
    "\n",
    "    # Combine building attributes into a batched format\n",
    "    batched_building_attributes = {}\n",
    "    for key in building_attributes_list[0].keys():\n",
    "        key_tensor_list = [d[key] for d in building_attributes_list]\n",
    "        # Since batch size is 1, we can directly extract the single tensor\n",
    "        # If batch size were greater than 1, we would use torch.stack(key_tensor_list)\n",
    "        batched_building_attributes[key] = key_tensor_list[0]\n",
    "\n",
    "    return gm_data_batch, batched_building_attributes, acc_floor_response_batch, blg_damage_state_batch\n",
    "\n",
    "# Now, use the custom collate function with the DataLoader\n",
    "# dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# 计算训练集和测试集的大小\n",
    "train_size = int(0.8 * len(dataset))\n",
    "validation_size = len(dataset) - train_size\n",
    "\n",
    "# 使用random_split来随机分割数据集\n",
    "train_dataset, validation_dataset = random_split(dataset, [train_size, validation_size])\n",
    "\n",
    "# 创建两个DataLoader实例\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def test_dataloader(dataloader):\n",
    "#     # Get a single batch from the dataloader\n",
    "#     for gm_data, building_attributes, acc_floor_response, blg_damage_state in dataloader:\n",
    "#         # Loop through all items in the batch\n",
    "#         for i in range(len(gm_data)):\n",
    "#             gm_data_sample = gm_data[i].squeeze()  # Remove unneeded dimensions\n",
    "#             acc_floor_response_sample = acc_floor_response[i].squeeze()\n",
    "#             blg_damage_state_sample = blg_damage_state[i].item()\n",
    "\n",
    "#             # Plot ground motion data\n",
    "#             plt.figure(figsize=(12, 4))\n",
    "#             plt.plot(gm_data_sample.cpu().numpy())\n",
    "#             plt.title('Ground Motion Data')\n",
    "#             plt.xlabel('Time Steps')\n",
    "#             plt.ylabel('Acceleration')\n",
    "#             plt.show()\n",
    "\n",
    "#             # Plot top floor acceleration response\n",
    "#             plt.figure(figsize=(12, 4))\n",
    "#             plt.plot(acc_floor_response_sample.cpu().numpy())\n",
    "#             plt.title('Top Floor Acceleration Response ')\n",
    "#             plt.xlabel('Time Steps')\n",
    "#             plt.ylabel('Acceleration')\n",
    "#             plt.show()\n",
    "\n",
    "#             # Print building attributes\n",
    "#             print('Building Attributes:', building_attributes)\n",
    "\n",
    "#             # Print building damage state\n",
    "#             print('Building Damage State:', blg_damage_state_sample)\n",
    "\n",
    "#         # For demonstration, we only process the first batch\n",
    "#         break\n",
    "\n",
    "# # Create DataLoader with custom collate function if needed\n",
    "# # dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "\n",
    "# # Test the dataloader\n",
    "# test_dataloader(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build SeT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PythonScripts.transformer import SeismicTransformerV4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SeismicTransformerV4(len_gm=3000,\n",
    "                              patch_size=250,\n",
    "                              hidden_size=768,\n",
    "                              num_heads=12,\n",
    "                              num_layers=12,\n",
    "                              dropout_attn=0.1,\n",
    "                              dropout_mlp=0.1,\n",
    "                              dropout_embed=0.1,\n",
    "                              num_of_classes=5).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_padding_mask must be match the sequence = 15\n",
    "key_padding_mask = torch.zeros(BATCH_SIZE, 15, dtype=torch.bool).to(device)\n",
    "attn_mask = torch.triu(torch.ones(12, 12), diagonal=1).bool().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 3000, 1]),\n",
       " {'IM': tensor([2], device='cuda:0'),\n",
       "  'height': tensor([15.], device='cuda:0'),\n",
       "  'stories': tensor([5], device='cuda:0'),\n",
       "  'struct_type': tensor([0], device='cuda:0')},\n",
       " torch.Size([256, 3000, 1]),\n",
       " torch.Size([256, 1]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm_data, building_attributes, acc_floor_response, blg_damage_state = next(iter(train_dataloader))\n",
    "\n",
    "gm_data = gm_data.to(device)\n",
    "acc_floor_response = acc_floor_response.to(device)\n",
    "blg_damage_state = blg_damage_state.to(device)\n",
    "\n",
    "gm_data.shape, building_attributes, acc_floor_response.shape, blg_damage_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_state, dynamic_response = model(encoder_input=gm_data, \n",
    "                                        struct_info=building_attributes,\n",
    "                                        decoder_input=acc_floor_response, \n",
    "                                        key_padding_mask=key_padding_mask, \n",
    "                                        attn_mask=attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 5]), torch.Size([256, 3000, 1]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "damage_state.shape, dynamic_response.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the SeT-4 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "loss_fn_classification = CrossEntropyLoss()\n",
    "loss_fn_regression = MSELoss()\n",
    "\n",
    "# tring adamW in SeT-3\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.0)\n",
    "# optimizer = torch.optim.Adam(params=SeismicTransformerV3_instance.parameters(), \n",
    "#                              lr=0.001,\n",
    "#                              betas=(0.9, 0.999),\n",
    "#                              weight_decay=0.0)\n",
    "\n",
    "NUM_EPOCH = 5\n",
    "\n",
    "num_training_steps = (198018*30 / BATCH_SIZE) * NUM_EPOCH         # total steps = len(train_dataset) / batch_size * epochs\n",
    "num_warmup_steps = num_training_steps * 0.2  # warmup_ratio usually is 20% of the total steps\n",
    "\n",
    "lr_scheduler_warmup = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                      num_warmup_steps=num_warmup_steps,\n",
    "                                                      num_training_steps=num_training_steps)\n",
    "\n",
    "# Set up the learning rate scheduler for decay, work inside train()\n",
    "lr_scheduler_decay = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                                mode='max', # set the min or max lr\n",
    "                                                                patience=10, # how many epoch loss don't change\n",
    "                                                                factor=0.1, # new_lr = old_lr * factor\n",
    "                                                                threshold=0.1, # loss change\n",
    "                                                                threshold_mode='rel', # compare mode\n",
    "                                                                cooldown=10, # how many epoch to wait\n",
    "                                                                min_lr=1e-7, # minimun of lr\n",
    "                                                                verbose=True)     # print something if useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PythonScripts.utility import CreateOutputFolder, CreateLogFileV3, CountNumOfTraining\n",
    "\n",
    "CLASSIFICATION_WEIGHT = 0.2\n",
    "HIDDEN_SIZE = 768\n",
    "NUM_HEADS = 24\n",
    "NUM_LAYERS = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of csv file \"training_results.csv\"\n",
    "num_of_training = CountNumOfTraining()\n",
    "\n",
    "# Create output folder\n",
    "save_dir = CreateOutputFolder(num_of_training=num_of_training+1,\n",
    "                              hidden_size=HIDDEN_SIZE,\n",
    "                              num_of_layer=NUM_LAYERS,\n",
    "                              num_of_head=NUM_HEADS,\n",
    "                              num_of_epoch=NUM_EPOCH)\n",
    "\n",
    "# Create log file, a csv file                  \n",
    "log_filename = CreateLogFileV3(save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df9fce8618744bdeadee4d01f59cdebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 : teacher forcing ratio = 1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mi:\\BaiduSyncdisk\\Project\\SeismicTransformer\\SeT_4_Trainbook.ipynb Cell 21\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/i%3A/BaiduSyncdisk/Project/SeismicTransformer/SeT_4_Trainbook.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Caculate the start time of the training\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/i%3A/BaiduSyncdisk/Project/SeismicTransformer/SeT_4_Trainbook.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m strat_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/i%3A/BaiduSyncdisk/Project/SeismicTransformer/SeT_4_Trainbook.ipynb#X26sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m results \u001b[39m=\u001b[39m train_set4(model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m      <a href='vscode-notebook-cell:/i%3A/BaiduSyncdisk/Project/SeismicTransformer/SeT_4_Trainbook.ipynb#X26sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                     train_loader\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[0;32m      <a href='vscode-notebook-cell:/i%3A/BaiduSyncdisk/Project/SeismicTransformer/SeT_4_Trainbook.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                     val_loader\u001b[39m=\u001b[39;49mvalidation_dataloader,\n\u001b[0;32m     <a href='vscode-notebook-cell:/i%3A/BaiduSyncdisk/Project/SeismicTransformer/SeT_4_Trainbook.ipynb#X26sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                     loss_fn_classification\u001b[39m=\u001b[39;49mloss_fn_classification,\n\u001b[0;32m     <a href='vscode-notebook-cell:/i%3A/BaiduSyncdisk/Project/SeismicTransformer/SeT_4_Trainbook.ipynb#X26sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                     loss_fn_regression\u001b[39m=\u001b[39;49mloss_fn_regression,\n\u001b[0;32m     <a href='vscode-notebook-cell:/i%3A/BaiduSyncdisk/Project/SeismicTransformer/SeT_4_Trainbook.ipynb#X26sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                     loss_fn_weight_classification\u001b[39m=\u001b[39;49mCLASSIFICATION_WEIGHT,\n\u001b[0;32m     <a href='vscode-notebook-cell:/i%3A/BaiduSyncdisk/Project/SeismicTransformer/SeT_4_Trainbook.ipynb#X26sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m                     optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[0;32m     <a href='vscode-notebook-cell:/i%3A/BaiduSyncdisk/Project/SeismicTransformer/SeT_4_Trainbook.ipynb#X26sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m                     lr_scheduler_warmup\u001b[39m=\u001b[39;49mlr_scheduler_warmup,\n\u001b[0;32m     <a href='vscode-notebook-cell:/i%3A/BaiduSyncdisk/Project/SeismicTransformer/SeT_4_Trainbook.ipynb#X26sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m                     lr_scheduler_decay\u001b[39m=\u001b[39;49mlr_scheduler_decay,\n\u001b[0;32m     <a href='vscode-notebook-cell:/i%3A/BaiduSyncdisk/Project/SeismicTransformer/SeT_4_Trainbook.ipynb#X26sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m                     num_warmup_steps\u001b[39m=\u001b[39;49mnum_warmup_steps,\n\u001b[0;32m     <a href='vscode-notebook-cell:/i%3A/BaiduSyncdisk/Project/SeismicTransformer/SeT_4_Trainbook.ipynb#X26sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m                     num_epochs\u001b[39m=\u001b[39;49mNUM_EPOCH,\n\u001b[0;32m     <a href='vscode-notebook-cell:/i%3A/BaiduSyncdisk/Project/SeismicTransformer/SeT_4_Trainbook.ipynb#X26sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m                     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m     <a href='vscode-notebook-cell:/i%3A/BaiduSyncdisk/Project/SeismicTransformer/SeT_4_Trainbook.ipynb#X26sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m                     log_filename\u001b[39m=\u001b[39;49mlog_filename)\n\u001b[0;32m     <a href='vscode-notebook-cell:/i%3A/BaiduSyncdisk/Project/SeismicTransformer/SeT_4_Trainbook.ipynb#X26sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     <a href='vscode-notebook-cell:/i%3A/BaiduSyncdisk/Project/SeismicTransformer/SeT_4_Trainbook.ipynb#X26sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m total_time \u001b[39m=\u001b[39m end_time \u001b[39m-\u001b[39m strat_time\n",
      "File \u001b[1;32mi:\\BaiduSyncdisk\\Project\\SeismicTransformer\\PythonScripts\\train.py:1014\u001b[0m, in \u001b[0;36mtrain_set4\u001b[1;34m(model, train_loader, val_loader, loss_fn_classification, loss_fn_regression, loss_fn_weight_classification, optimizer, lr_scheduler_warmup, lr_scheduler_decay, num_warmup_steps, num_epochs, device, log_filename)\u001b[0m\n\u001b[0;32m   1011\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch 00\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m : teacher forcing ratio = \u001b[39m\u001b[39m{\u001b[39;00mteacher_forcing_ratio\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1013\u001b[0m \u001b[39m# train step\u001b[39;00m\n\u001b[1;32m-> 1014\u001b[0m train_loss, train_acc, train_mse \u001b[39m=\u001b[39m train_step_set4(model, \n\u001b[0;32m   1015\u001b[0m                                                    train_loader,\n\u001b[0;32m   1016\u001b[0m                                                    loss_fn_classification,\n\u001b[0;32m   1017\u001b[0m                                                    loss_fn_regression,\n\u001b[0;32m   1018\u001b[0m                                                    loss_fn_weight_classification,\n\u001b[0;32m   1019\u001b[0m                                                    optimizer,\n\u001b[0;32m   1020\u001b[0m                                                    lr_scheduler_warmup,\n\u001b[0;32m   1021\u001b[0m                                                    num_warmup_steps,\n\u001b[0;32m   1022\u001b[0m                                                    teacher_forcing_ratio,\n\u001b[0;32m   1023\u001b[0m                                                    device)\n\u001b[0;32m   1025\u001b[0m \u001b[39m# if train loss = nan, break\u001b[39;00m\n\u001b[0;32m   1026\u001b[0m \u001b[39mif\u001b[39;00m math\u001b[39m.\u001b[39misnan(train_loss):\n",
      "File \u001b[1;32mi:\\BaiduSyncdisk\\Project\\SeismicTransformer\\PythonScripts\\train.py:862\u001b[0m, in \u001b[0;36mtrain_step_set4\u001b[1;34m(model, dataloader, loss_fn_classification, loss_fn_regression, loss_fn_weight_classification, optimizer, lr_scheduler_warmup, num_warmup_steps, teacher_forcing_ratio, device)\u001b[0m\n\u001b[0;32m    860\u001b[0m \u001b[39m# Zero gradients, backward pass, and optimizer step\u001b[39;00m\n\u001b[0;32m    861\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m--> 862\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    863\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m    865\u001b[0m \u001b[39m# Update learning rate scheduler\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ml\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ml\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from PythonScripts.train import train_set4\n",
    "import time\n",
    "\n",
    "# Caculate the start time of the training\n",
    "strat_time = time.time()\n",
    "\n",
    "results = train_set4(model=model,\n",
    "                    train_loader=train_dataloader,\n",
    "                    val_loader=validation_dataloader,\n",
    "                    loss_fn_classification=loss_fn_classification,\n",
    "                    loss_fn_regression=loss_fn_regression,\n",
    "                    loss_fn_weight_classification=CLASSIFICATION_WEIGHT,\n",
    "                    optimizer=optimizer,\n",
    "                    lr_scheduler_warmup=lr_scheduler_warmup,\n",
    "                    lr_scheduler_decay=lr_scheduler_decay,\n",
    "                    num_warmup_steps=num_warmup_steps,\n",
    "                    num_epochs=NUM_EPOCH,\n",
    "                    device=device,\n",
    "                    log_filename=log_filename)\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - strat_time\n",
    "print(f\"Training time: {total_time:.3f}秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
