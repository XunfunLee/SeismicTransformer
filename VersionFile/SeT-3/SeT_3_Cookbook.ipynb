{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seismic Transformer V3.0 cookbook\n",
    "\n",
    "*Author: Jason Jiang (Xunfun Lee)*\n",
    "\n",
    "*Date: 2023.12.31*\n",
    "\n",
    "> **Note:** This is a cookbook for Seismic Transformer V3.0. It has a vast differece with the previous version. I almost rewrite all the code including MLP Block, MHA Block, `train_step()`, `validation_step()`, `train()`, `test()` and so on, so make sure you are reading the corret version of them inside PythonScripts. \n",
    "> The reason is that SeT-3 is more complicated than SeT-1 and SeT-2 after adding the decoder and splicer into the model. In SeT-3 I also update the mask process(for the reason that it is totally a mess in SeT-2 and there are two masks in SeT-3 rather than 1 mask in SeT-2), frequency, token embedding(more elegant). In conclusion, it is a fresh new and efficient model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "from PythonScripts.utility import SetDevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: cuda\n",
      "CUDA device numbers:  1\n"
     ]
    }
   ],
   "source": [
    "device = SetDevice()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP block class\n",
    "class MLPBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hidden_size: int = 768, \n",
    "                 fc_hidden_size: int = 3072, \n",
    "                 dropout_rate: float = 0.1):\n",
    "\n",
    "        super(MLPBlock, self).__init__()\n",
    "        \n",
    "        # Pre-Layer Normalization\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        self.linear1 = nn.Linear(hidden_size, fc_hidden_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.linear2 = nn.Linear(fc_hidden_size, hidden_size)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Save the residual\n",
    "        residual = x\n",
    "        # Apply pre-layer normalization\n",
    "        x = self.layer_norm(x)\n",
    "        # First fully connected layer\n",
    "        x = self.linear1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.dropout1(x)\n",
    "        # Second fully connected layer\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout2(x)\n",
    "        # Add the residual\n",
    "        x = x + residual\n",
    "        # Implementing the residual connection\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test functions\n",
    "def test_mlp_block_summary(mlp_block_instance:nn.Module,\n",
    "                           input_data:torch.Tensor):\n",
    "    # Using torchinfo to summarize the model\n",
    "    print(summary(mlp_block_instance, input_data=input_data))\n",
    "\n",
    "def test_mlp_block_forward(mlp_block_instance, input_tensor):\n",
    "    # Forward pass and print output shape\n",
    "    output = mlp_block_instance(input_tensor)\n",
    "    print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing MLPBlock with torchinfo.summary:\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "MLPBlock                                 [64, 14, 768]             --\n",
      "├─LayerNorm: 1-1                         [64, 14, 768]             1,536\n",
      "├─Linear: 1-2                            [64, 14, 3072]            2,362,368\n",
      "├─GELU: 1-3                              [64, 14, 3072]            --\n",
      "├─Dropout: 1-4                           [64, 14, 3072]            --\n",
      "├─Linear: 1-5                            [64, 14, 768]             2,360,064\n",
      "├─Dropout: 1-6                           [64, 14, 768]             --\n",
      "==========================================================================================\n",
      "Total params: 4,723,968\n",
      "Trainable params: 4,723,968\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 302.33\n",
      "==========================================================================================\n",
      "Input size (MB): 2.75\n",
      "Forward/backward pass size (MB): 33.03\n",
      "Params size (MB): 18.90\n",
      "Estimated Total Size (MB): 54.68\n",
      "==========================================================================================\n",
      "\n",
      "Testing MLPBlock with forward pass:\n",
      "Output shape: torch.Size([64, 14, 768])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create MLP block instance for testing\n",
    "mlp_block_instance = MLPBlock().to(device)\n",
    "\n",
    "# Generate a random tensor for testing\n",
    "input_tensor_for_testing = torch.rand(batch_size, 14, 768).to(device)  # Batch size of 14\n",
    "\n",
    "# Call test functions\n",
    "print(\"Testing MLPBlock with torchinfo.summary:\")\n",
    "test_mlp_block_summary(mlp_block_instance, input_tensor_for_testing)\n",
    "\n",
    "print(\"\\nTesting MLPBlock with forward pass:\")\n",
    "test_mlp_block_forward(mlp_block_instance, input_tensor_for_testing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MHA Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHABlock(nn.Module):\n",
    "    \"\"\"Multi-head attention block class, can be used in the Encoder, Decoder, and Cross-Attention parts of the Transformer model.\n",
    "\n",
    "    Args:\n",
    "        hidden_size (int): Hidden size of the input tensor. Defaults to 768.\n",
    "        num_heads (int): Number of attention heads. Defaults to 12.\n",
    "        dropout_attn (float): Dropout rate. Defaults to 0.1.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 hidden_size: int = 768, \n",
    "                 num_heads: int = 12, \n",
    "                 dropout_attn: float = 0.1,\n",
    "                 batch_first: bool = True):\n",
    "        \n",
    "        super(MHABlock, self).__init__()\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=hidden_size, \n",
    "                                                    num_heads=num_heads, \n",
    "                                                    dropout=dropout_attn,\n",
    "                                                    batch_first=batch_first)\n",
    "        self.dropout = nn.Dropout(dropout_attn)\n",
    "\n",
    "    def forward(self, query, key, value, key_padding_mask=None, attn_mask=None, need_weights=True):\n",
    "        # Save the residual\n",
    "        residual = query\n",
    "        # Apply pre-layer normalization\n",
    "        normed_query = self.layer_norm(query)\n",
    "        normed_key = self.layer_norm(key)\n",
    "        normed_value = self.layer_norm(value)\n",
    "\n",
    "        # Multi-head attention\n",
    "        attn_output, attn_output_weights = self.multihead_attn(normed_query,\n",
    "                                                               normed_key,\n",
    "                                                               normed_value,\n",
    "                                                               key_padding_mask=key_padding_mask,\n",
    "                                                               attn_mask=attn_mask,\n",
    "                                                               need_weights=need_weights)\n",
    "        # Apply dropout\n",
    "        attn_output = self.dropout(attn_output)\n",
    "        # Add the residual\n",
    "        attn_output = attn_output + residual\n",
    "        \n",
    "        return attn_output, attn_output_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MHABlock(\n",
       "   (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "   (multihead_attn): MultiheadAttention(\n",
       "     (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "   )\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " MHABlock(\n",
       "   (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "   (multihead_attn): MultiheadAttention(\n",
       "     (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "   )\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " MHABlock(\n",
       "   (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "   (multihead_attn): MultiheadAttention(\n",
       "     (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "   )\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate three different instances of MHABlock for encoder, masked MHA (typically used in decoder), and cross MHA.\n",
    "\n",
    "# Instance for the encoder's MHA\n",
    "encoder_mha = MHABlock(hidden_size=768, num_heads=12, dropout_attn=0.1, batch_first=True)\n",
    "\n",
    "# Instance for the decoder's masked MHA\n",
    "# For masked MHA in the decoder, we need to ensure that the attention mechanism \n",
    "# does not attend to subsequent positions. This is typically handled outside the MHABlock,\n",
    "# by providing an appropriate mask to the 'forward' method.\n",
    "decoder_masked_mha = MHABlock(hidden_size=768, num_heads=12, dropout_attn=0.1, batch_first=True)\n",
    "\n",
    "# Instance for the cross MHA (used in decoder to attend over encoder's outputs)\n",
    "cross_mha = MHABlock(hidden_size=768, num_heads=12, dropout_attn=0.1, batch_first=True)\n",
    "\n",
    "(encoder_mha, decoder_masked_mha, cross_mha)  # Return the instances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test functions for each type of MHABlock instance with appropriate masks\n",
    "\n",
    "def test_encoder_mha(encoder_mha_instance, input_tensor):\n",
    "    \"\"\"\n",
    "    Test the encoder's MHABlock instance. \n",
    "    The encoder does not require a special mask for self-attention.\n",
    "    \"\"\"\n",
    "    print(\"Testing Encoder's MHABlock with forward pass:\")\n",
    "    output, _ = encoder_mha_instance(input_tensor, input_tensor, input_tensor)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    return output\n",
    "\n",
    "def test_encoder_mha_with_padding_mask(encoder_mha_instance, input_tensor, padding_mask):\n",
    "    \"\"\"\n",
    "    Test the encoder's MHABlock instance with a padding mask.\n",
    "    The padding mask is used to ignore padded positions in the input sequence.\n",
    "    \"\"\"\n",
    "    print(\"Testing Encoder's MHABlock with Padding Mask and forward pass:\")\n",
    "    output, _ = encoder_mha_instance(input_tensor, input_tensor, input_tensor, key_padding_mask=padding_mask)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    return output\n",
    "\n",
    "def test_decoder_masked_mha(decoder_masked_mha_instance, input_tensor, attn_mask):\n",
    "    \"\"\"\n",
    "    Test the decoder's masked MHABlock instance with a correctly shaped attention mask.\n",
    "    The mask is used to prevent positions from attending to subsequent positions.\n",
    "    \"\"\"\n",
    "    print(\"Testing Decoder's Masked MHABlock with Correctly Shaped Attn Mask and forward pass:\")\n",
    "    output, _ = decoder_masked_mha_instance(input_tensor, input_tensor, input_tensor, attn_mask=attn_mask)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    return output\n",
    "\n",
    "def test_cross_mha(cross_mha_instance, input_tensor, memory_tensor):\n",
    "    \"\"\"\n",
    "    Test the cross MHABlock instance. \n",
    "    The cross-attention does not require a special mask in this context.\n",
    "    \"\"\"\n",
    "    print(\"Testing Cross MHABlock with forward pass:\")\n",
    "    output, _ = cross_mha_instance(input_tensor, memory_tensor, memory_tensor)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test encoder MHA\n",
    "\n",
    "key_padding_mask.size = torch.Size([batch_size, seq_len]) (dtype = torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Encoder's MHABlock with forward pass:\n",
      "Output shape: torch.Size([64, 14, 768])\n",
      "Testing Encoder's MHABlock with Padding Mask and forward pass:\n",
      "Output shape: torch.Size([64, 14, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 12, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input = torch.rand(batch_size, 14, 768)  # (L, N, E) for encoder\n",
    "output_encoder = test_encoder_mha(encoder_mha, encoder_input)\n",
    "\n",
    "padding_mask = torch.zeros(batch_size, 14, dtype=torch.bool)\n",
    "output_encoder_mask = test_encoder_mha_with_padding_mask(encoder_mha, encoder_input, padding_mask)\n",
    "\n",
    "output_encoder[:,1:13,:].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test decoder: masked MHA\n",
    "\n",
    "attn_mask = torch.triu(torch.ones(seq_length, seq_length), diagonal=1).bool()\n",
    "\n",
    "`torch.triu` 是“triangular upper”的缩写，表示生成一个上三角矩阵。\n",
    "\n",
    "参数 `diagonal=1` 表示对角线上方一行的元素（而不是对角线上的元素）开始保持原有的值（在这里是1），对角线及以下的元素设置为0。\n",
    "\n",
    "`.bool()` 将上述上三角矩阵转换为布尔类型，其中1变成 `True` ，0变成 `False` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Decoder's Masked MHABlock with Correctly Shaped Attn Mask and forward pass:\n",
      "Output shape: torch.Size([64, 12, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 12, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform tests\n",
    "decoder_input = torch.rand(batch_size, 12, 768)  # (L, N, E) for decoder\n",
    "\n",
    "seq_length = decoder_input.shape[1]\n",
    "attn_mask = torch.triu(torch.ones(seq_length, seq_length), diagonal=1).bool()\n",
    "output_decoder = test_decoder_masked_mha(decoder_masked_mha, decoder_input, attn_mask)\n",
    "\n",
    "output_decoder.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test decoder: cross attention MHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Cross MHABlock with forward pass:\n",
      "Output shape: torch.Size([64, 12, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 12, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_tensor = output_encoder[:,1:13,:]\n",
    "output_cross = test_cross_mha(cross_mha, output_decoder, memory_tensor)\n",
    "output_cross.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch Embedding Block (PE Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class to perform the operations on the left side of the diagram (slice, linear)\n",
    "class PatchEmbeddingBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 len_gm:int=3000, \n",
    "                 patch_size:int=250, \n",
    "                 output_size:int=768):\n",
    "        super(PatchEmbeddingBlock, self).__init__()\n",
    "\n",
    "        # Assume that slice_size is a two-dimensional tuple (seq_len, features)\n",
    "        self.patch_size = patch_size\n",
    "        self.output_size = output_size\n",
    "        self.linear = nn.Linear(patch_size, output_size)\n",
    "        self.num_of_patches = len_gm // patch_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # verify the input shape\n",
    "        assert x.shape[1] == self.num_of_patches * self.patch_size, \\\n",
    "            f'Input sequence length should be {self.num_of_patches * self.patch_size}'\n",
    "        \n",
    "        # [batch_size, sequence_length, 1] --> [batch_size, num_patches, patch_size]\n",
    "        # e.g. [64, 3000, 1] --> [64, 12, 250]\n",
    "        x = x.view(-1, self.num_of_patches, self.patch_size)\n",
    "\n",
    "        # Reshape for the linear layer\n",
    "        x = self.linear(x)\n",
    "        # Reshape to the desired output size (batch_size, seq_len, output_size)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_PEBlock(PEBlock_instance, input_tensor):\n",
    "    \"\"\"\n",
    "    Test the cross MHABlock instance. \n",
    "    The cross-attention does not require a special mask in this context.\n",
    "    \"\"\"\n",
    "    print(\"Testing Cross MHABlock with forward pass:\")\n",
    "    output = PEBlock_instance(input_tensor)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3000, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PEBlock_Instance = PatchEmbeddingBlock().to(device)\n",
    "input_PE = torch.rand(batch_size, 3000, 1).to(device)\n",
    "input_PE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Cross MHABlock with forward pass:\n",
      "Output shape: torch.Size([64, 12, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 12, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_PE = test_PEBlock(PEBlock_Instance, input_PE)\n",
    "output_PE.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Embedding Block (FE Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "class FreqEmbeddingBlock(nn.Module):\n",
    "  def __init__(self, \n",
    "               conv_output_size:int=750, \n",
    "               linear_output_size:int=768):\n",
    "    \n",
    "    super(FreqEmbeddingBlock, self).__init__()\n",
    "\n",
    "    self.fft = np.fft.rfft\n",
    "    self.conv1d = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=2, stride=2)\n",
    "    self.linear = nn.Linear(conv_output_size, linear_output_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # FFT\n",
    "    x = torch.fft.rfft(x).real\n",
    "    # Keep only the first 1500 elements (as FFT will return N/2+1 elements for real input)\n",
    "    x = x[:, :1500, :]\n",
    "    # Convolution\n",
    "    x = x.permute(0, 2, 1)\n",
    "    x = self.conv1d(x)\n",
    "    # Linear transformation\n",
    "    x = self.linear(x)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEBlock_Instance = FreqEmbeddingBlock().to(device)\n",
    "input_FE = torch.rand(64, 3000, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_FEBlock(FEBlock_Instance, input_tensor):\n",
    "    \"\"\"\n",
    "    Test the cross MHABlock instance. \n",
    "    The cross-attention does not require a special mask in this context.\n",
    "    \"\"\"\n",
    "    print(\"Testing Cross MHABlock with forward pass:\")\n",
    "    output = FEBlock_Instance(input_tensor)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Cross MHABlock with forward pass:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([64, 1, 768])\n"
     ]
    }
   ],
   "source": [
    "output = test_FEBlock(FEBlock_Instance, input_FE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hidden_size: int = 768, \n",
    "                 num_heads: int = 12, \n",
    "                 fc_hidden_size: int = 3072, \n",
    "                 dropout_attn: float = 0.1,\n",
    "                 dropout_mlp: float = 0.1,\n",
    "                 batch_first: bool = True):\n",
    "        \n",
    "        super(EncoderBlock, self).__init__()\n",
    "        \n",
    "        self.mha_block = MHABlock(hidden_size=hidden_size, \n",
    "                                  num_heads=num_heads, \n",
    "                                  dropout_attn=dropout_attn,\n",
    "                                  batch_first=batch_first)\n",
    "        \n",
    "        self.mlp_block = MLPBlock(hidden_size=hidden_size, \n",
    "                                  fc_hidden_size=fc_hidden_size, \n",
    "                                  dropout_rate=dropout_mlp)\n",
    "        \n",
    "    def forward(self, x, key_padding_mask=None, need_weights=True):\n",
    "        # Multi-head attention block\n",
    "        x, attn_weights = self.mha_block(x, x, x, key_padding_mask=key_padding_mask, need_weights=need_weights)\n",
    "        # MLP block\n",
    "        x = self.mlp_block(x)\n",
    "\n",
    "        return x, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_EBBlock(EncoderBlock_Instance, input_tensor):\n",
    "    \"\"\"\n",
    "    Test the cross MHABlock instance. \n",
    "    The cross-attention does not require a special mask in this context.\n",
    "    \"\"\"\n",
    "    print(\"Testing Cross MHABlock with forward pass:\")\n",
    "    output, attn_weights = EncoderBlock_Instance(input_tensor)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "EncoderBlock_Instance = EncoderBlock().to(device)\n",
    "input_EB = torch.rand(64, 14, 768).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Cross MHABlock with forward pass:\n",
      "Output shape: torch.Size([64, 14, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 14, 768]), torch.Size([64, 14, 14]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, attn_weights = test_EBBlock(EncoderBlock_Instance, input_EB)\n",
    "output.shape, attn_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderV1(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 len_gm:int=3000,\n",
    "                 patch_size:int=250,\n",
    "                 hidden_size:int=768,\n",
    "                 num_heads:int=12,\n",
    "                 num_layers:int=12,\n",
    "                 dropout_attn:float=0.1,\n",
    "                 dropout_mlp:float=0.1,\n",
    "                 dropout_embed:float=0.1):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Calculate the number of patches\n",
    "        self.num_of_patch = len_gm // patch_size\n",
    "\n",
    "        # Initialize a variable to stroe the attention weights\n",
    "        self.attention_weights_list = []  # Initialize it here\n",
    "        \n",
    "        # BLOCK\n",
    "        # patch embedding\n",
    "        self.PatchEmbedding = PatchEmbeddingBlock(len_gm=len_gm,\n",
    "                                      patch_size=patch_size,\n",
    "                                      output_size=hidden_size)\n",
    "        \n",
    "        # frequency embedding\n",
    "        self.FreqEmbedding = FreqEmbeddingBlock(conv_output_size=len_gm // 2 // 2,         # default is 750\n",
    "                                     linear_output_size=hidden_size)\n",
    "\n",
    "        # encoder layer\n",
    "        self.EncoderLayers = nn.Sequential(*[EncoderBlock(hidden_size=hidden_size,\n",
    "                                                          num_heads=num_heads,\n",
    "                                                          fc_hidden_size=hidden_size*4,\n",
    "                                                          dropout_attn=dropout_attn,\n",
    "                                                          dropout_mlp=dropout_mlp) for _ in range(num_layers)])\n",
    "\n",
    "        # [TOKEN]\n",
    "        # [TIME] - time token\n",
    "        self.time_token = nn.Parameter(torch.randn(1, 1, hidden_size),\n",
    "                                       requires_grad=True)  # trainable parameter\n",
    "\n",
    "        # [FREQ] - frequency token\n",
    "        self.freq_token = nn.Parameter(torch.randn(1, 1, hidden_size),\n",
    "                                       requires_grad=True)  # trainable parameter\n",
    "\n",
    "        # [CLS] - class token\n",
    "        self.class_token = nn.Parameter(torch.randn(1, 1, hidden_size),\n",
    "                                        requires_grad=True)  # trainable parameter\n",
    "\n",
    "        # POSITION\n",
    "        # positional embedding\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(1, self.num_of_patch+2, hidden_size),\n",
    "                                                  requires_grad=True)  # trainable parameter\n",
    "        \n",
    "        # Dropout\n",
    "        self.embedding_dropout = nn.Dropout(dropout_embed)\n",
    "\n",
    "\n",
    "    def forward(self, x, key_padding_mask=None, need_weights=True):\n",
    "\n",
    "        # Get the batch size\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # clear the attention weights list\n",
    "        self.attention_weights_list = []\n",
    "\n",
    "        # patch embedding\n",
    "        time_sequence = self.PatchEmbedding(x)\n",
    "\n",
    "        # [TIME] token\n",
    "        time_tokens = self.time_token.repeat(batch_size, self.num_of_patch, 1)\n",
    "\n",
    "        # concatenate the time sequence with the time tokens\n",
    "        time_sequence_with_token = time_sequence + time_tokens                                      # [batch_size, 12, hidden_size]\n",
    "\n",
    "        # frequency embedding\n",
    "        freq_sequence = self.FreqEmbedding(x)\n",
    "\n",
    "        # [FREQ] token\n",
    "        freq_tokens = self.freq_token.repeat(batch_size, 1, 1)\n",
    "\n",
    "        # concatenate the frequency sequence with the frequency tokens\n",
    "        freq_sequence_with_token = freq_sequence + freq_tokens                                      # [batch_size, 1, hidden_size]\n",
    "\n",
    "        # cat the time sequence and the frequency sequence\n",
    "        sequence_combine = torch.cat((time_sequence_with_token, freq_sequence_with_token), dim=1)   # [batch_size, 13, hidden_size]\n",
    "\n",
    "        # [CLS] token\n",
    "        class_tokens = self.class_token.expand(batch_size, -1, -1) # \"-1\" means to infer the dimension (try this line on its own)\n",
    "\n",
    "        # concatenate the class token with the sequence\n",
    "        sequence_combine_with_cls = torch.cat((class_tokens, sequence_combine), dim=1)              # [batch_size, 14, hidden_size]\n",
    "\n",
    "        # embedding dropout\n",
    "        x = self.embedding_dropout(sequence_combine_with_cls)\n",
    "\n",
    "        # Encoder Layer\n",
    "        for layer in self.EncoderLayers:\n",
    "            x, attn_weights = layer(x, key_padding_mask=key_padding_mask, need_weights=need_weights)\n",
    "            self.attention_weights_list.append(attn_weights)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_Encoder(Encoder_Instance, input_tensor):\n",
    "    \"\"\"\n",
    "    Test the cross MHABlock instance. \n",
    "    The cross-attention does not require a special mask in this context.\n",
    "    \"\"\"\n",
    "    print(\"Testing Cross MHABlock with forward pass:\")\n",
    "    output = Encoder_Instance(input_tensor)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "EncoderV1_Instance = EncoderV1().to(device)\n",
    "input_Encoder = torch.rand(64, 3000, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Cross MHABlock with forward pass:\n",
      "Output shape: torch.Size([64, 14, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 14, 768])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = test_Encoder(EncoderV1_Instance, input_Encoder)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hidden_size: int = 768, \n",
    "                 num_heads: int = 12, \n",
    "                 fc_hidden_size: int = 3072, \n",
    "                 dropout_attn: float = 0.1,\n",
    "                 dropout_mlp: float = 0.1,\n",
    "                 batch_first: bool = True):\n",
    "        \n",
    "        super(DecoderBlock, self).__init__()\n",
    "        \n",
    "        self.mmha_block = MHABlock(hidden_size=hidden_size, \n",
    "                                  num_heads=num_heads, \n",
    "                                  dropout_attn=dropout_attn,\n",
    "                                  batch_first=batch_first)\n",
    "        \n",
    "        self.cmha_block = MHABlock(hidden_size=hidden_size, \n",
    "                                  num_heads=num_heads, \n",
    "                                  dropout_attn=dropout_attn,\n",
    "                                  batch_first=batch_first)\n",
    "        \n",
    "        self.mlp_block = MLPBlock(hidden_size=hidden_size, \n",
    "                                  fc_hidden_size=fc_hidden_size, \n",
    "                                  dropout_rate=dropout_mlp)\n",
    "        \n",
    "    def forward(self, query, key, value, output_encoder, attn_mask=None, need_weights=True):\n",
    "\n",
    "        # Masked Multi-head attention block\n",
    "        mmha_output, mmha_attn_weights = self.mmha_block(query, key, value, need_weights=need_weights)\n",
    "\n",
    "        # Cross Multi-head attention block\n",
    "        cmha_output, cmha_attn_weights = self.cmha_block(mmha_output, output_encoder, output_encoder, attn_mask=attn_mask, need_weights=need_weights)\n",
    "\n",
    "        # MLP block\n",
    "        output = self.mlp_block(cmha_output)\n",
    "\n",
    "        return output, mmha_attn_weights, cmha_attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderV1(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 len_gm:int=3000,\n",
    "                 patch_size:int=250,\n",
    "                 hidden_size:int=768,\n",
    "                 num_heads:int=12,\n",
    "                 num_layers:int=12,\n",
    "                 dropout_attn:float=0.1,\n",
    "                 dropout_mlp:float=0.1,\n",
    "                 dropout_embed:float=0.1,\n",
    "                 device:torch.device=\"cuda\"):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Calculate the number of patches\n",
    "        self.num_of_patch = len_gm // patch_size\n",
    "\n",
    "        # Initialize a variable to stroe the attention weights\n",
    "        self.mmha_attn_weights_list = []  # Initialize it here\n",
    "        self.cmha_attn_weights_list = []  # Initialize it here\n",
    "        \n",
    "        # BLOCK\n",
    "        # patch embedding\n",
    "        self.PatchEmbedding = PatchEmbeddingBlock(len_gm=len_gm,\n",
    "                                      patch_size=patch_size,\n",
    "                                      output_size=hidden_size)\n",
    "\n",
    "        # encoder layer\n",
    "        self.DecoderLayers = nn.Sequential(*[DecoderBlock(hidden_size=hidden_size,\n",
    "                                                          num_heads=num_heads,\n",
    "                                                          fc_hidden_size=hidden_size*4,\n",
    "                                                          dropout_attn=dropout_attn,\n",
    "                                                          dropout_mlp=dropout_mlp) for _ in range(num_layers)])\n",
    "\n",
    "        # POSITION\n",
    "        # positional embedding\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(1, self.num_of_patch, hidden_size),\n",
    "                                                  requires_grad=True)  # trainable parameter\n",
    "        \n",
    "        # Dropout\n",
    "        self.embedding_dropout = nn.Dropout(dropout_embed)\n",
    "\n",
    "        # Set device\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, output_encoder, target_sequence=None, attn_mask=None, need_weights=True):\n",
    "        # Check if target_sequence is provided\n",
    "        if target_sequence is not None:\n",
    "            # Training mode\n",
    "            # 使用target_sequence（目标序列）作为输入\n",
    "            x = target_sequence\n",
    "        else:\n",
    "            # Inference mode\n",
    "            # 初始化一个序列来逐步构建输出\n",
    "            x = self._init_sequence(batch_size=output_encoder.shape[0])\n",
    "\n",
    "        # Patch embedding and position encoding can be done here if needed\n",
    "        # patch embedding\n",
    "        time_sequence = self.PatchEmbedding(x)\n",
    "\n",
    "        time_sequence = time_sequence + self.positional_embedding\n",
    "\n",
    "        # embedding dropout\n",
    "        x = self.embedding_dropout(time_sequence)\n",
    "\n",
    "                # clear the attention weights list\n",
    "        self.mmha_attn_weights_list = []\n",
    "        self.cmha_attn_weights_list = []\n",
    "\n",
    "        # Encoder Layer\n",
    "        for layer in self.DecoderLayers:\n",
    "            x, mmha_attn_weights, cmha_attn_weights = layer(query=x, \n",
    "                                                            key=x,\n",
    "                                                            value=x,\n",
    "                                                            output_encoder=output_encoder,\n",
    "                                                            attn_mask=attn_mask, need_weights=need_weights)\n",
    "            # Store attention weights if needed\n",
    "            self.mmha_attn_weights_list.append(mmha_attn_weights)\n",
    "            self.cmha_attn_weights_list.append(cmha_attn_weights)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def generate_sequence(self, output_encoder, attn_mask=None):\n",
    "        # Initial sequence generation for inference mode\n",
    "        print(output_encoder.shape[0])\n",
    "        generated_sequence = self._init_sequence(batch_size=output_encoder.shape[0])\n",
    "        for _ in range(self.num_of_patch):\n",
    "            # Assume that you append to generated_sequence at each step\n",
    "            # You may need to modify this loop to match your actual sequence generation process\n",
    "            generated_sequence = self.forward(output_encoder, generated_sequence, attn_mask)\n",
    "        \n",
    "        return generated_sequence\n",
    "\n",
    "    def _init_sequence(self, batch_size):\n",
    "        # Initialize the sequence for the decoder to start generating the output\n",
    "        # This can be zeros, learned embeddings, or some form of encoder output processing\n",
    "        initial_sequence = torch.zeros((batch_size, 3000, 1)).to(self.device)\n",
    "        # Modify this to suit how you want to start sequence generation\n",
    "        return initial_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_Decoder(Decoder_Instance, input_decoder, output_encoder, attn_mask=None):\n",
    "    \"\"\"\n",
    "    Test the cross MHABlock instance. \n",
    "    The cross-attention does not require a special mask in this context.\n",
    "    \"\"\"\n",
    "    print(\"Testing Cross MHABlock with forward pass:\")\n",
    "    output = Decoder_Instance(output_encoder, input_decoder, attn_mask=attn_mask, need_weights=True)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DecoderV1_Instance = DecoderV1().to(device)\n",
    "input_decoder = torch.rand(64, 3000, 1).to(device)\n",
    "output_encoder = torch.rand(64, 12, 768).to(device)\n",
    "attn_mask = torch.triu(torch.ones(seq_length, seq_length), diagonal=1).bool().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Cross MHABlock with forward pass:\n",
      "Output shape: torch.Size([64, 12, 768])\n"
     ]
    }
   ],
   "source": [
    "# training mode, with target sequence\n",
    "output = test_Decoder(DecoderV1_Instance, input_decoder, output_encoder, attn_mask=attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Cross MHABlock with forward pass:\n",
      "Output shape: torch.Size([64, 12, 768])\n"
     ]
    }
   ],
   "source": [
    "# inference mode, generate sequence\n",
    "output = test_Decoder(DecoderV1_Instance, None, output_encoder, attn_mask=attn_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierV1(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 hidden_size:int=768,\n",
    "                 num_of_classes:int=5) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        # LN\n",
    "        self.LayerNorm = nn.LayerNorm(normalized_shape=hidden_size)\n",
    "        # Linear\n",
    "        self.Linear = nn.Linear(in_features=hidden_size, out_features=num_of_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # LN\n",
    "        x = self.LayerNorm(x)\n",
    "        # Linear                        [N, 768]\n",
    "        logits = self.Linear(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splicer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplicerV1(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 hidden_size:int=768,\n",
    "                 patch_size:int=250,\n",
    "                 len_gm:int=3000) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # LN\n",
    "        self.LayerNorm = nn.LayerNorm(normalized_shape=hidden_size)\n",
    "\n",
    "        # linear1: [batch_size, 12, 768] --> [batch_size, 12, 250]\n",
    "        self.Linear1 = nn.Linear(in_features=hidden_size, out_features=patch_size)\n",
    "\n",
    "        # linear2: [batch_size, 3000, 1] --> [batch_size, 3000, 1]\n",
    "        self.Linear2 = nn.Linear(in_features=len_gm, out_features=len_gm)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LN\n",
    "        x = self.LayerNorm(x)\n",
    "        # linear1 with GELU\n",
    "        x = F.gelu(self.Linear1(x))\n",
    "        # [N, 12, 250] --> [N, 3000]\n",
    "        x = x.view(x.size(0), -1)  \n",
    "        # linear2\n",
    "        x = self.Linear2(x)\n",
    "        # [N, 3000, 1]\n",
    "        x = x.view(x.size(0), -1, 1)\n",
    "\n",
    "        return x \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SplicerV1().to(device)\n",
    "input_tensor = torch.rand(64, 12, 768).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3000, 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(input_tensor)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_gm = torch.rand(64, 3000, 1).to(device)\n",
    "input_floorResponse = torch.rand(64, 3000, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "EncoderV1 = EncoderV1().to(device)\n",
    "DecoderV1 = DecoderV1().to(device)\n",
    "ClassifierV1 = ClassifierV1().to(device)\n",
    "SplicerV1 = SplicerV1().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_padding_mask = torch.zeros(64, 14, dtype=torch.bool).to(device)\n",
    "attn_mask = torch.triu(torch.ones(12, 12), diagonal=1).bool().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 14, 768])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_encoder = EncoderV1(input_gm, key_padding_mask=key_padding_mask)\n",
    "output_encoder.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 14, 5])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_classfier = ClassifierV1(output_encoder)\n",
    "output_classfier.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_decoder = torch.rand(64, 3000, 1).to(device)\n",
    "# output_decoder = DecoderV1(output_encoder[:,1:13,:], input_decoder, attn_mask=attn_mask)\n",
    "# output_decoder.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splicer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_splicer = SplicerV1(output_decoder)\n",
    "# output_splicer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** Haha, now our SeismicTransformer is ready to go! Let's build entire model the train step for it!\n",
    "\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "This is a multi task learning problem, so we need to define the loss function for each task.\n",
    "\n",
    "- Task 1: Classification (damage state of the building)\n",
    "\n",
    "- Task 2: Regression (dynamic response of the top floor)\n",
    "\n",
    "It is wise to create a big class containing all the module and caculate each loss within each task, but using a total loss and optimizer to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SeismicTransformer V3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeismicTransformerV3(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 len_gm:int=3000,\n",
    "                 patch_size:int=250,\n",
    "                 hidden_size:int=768,\n",
    "                 num_heads:int=12,\n",
    "                 num_layers:int=12,\n",
    "                 dropout_attn:float=0.1,\n",
    "                 dropout_mlp:float=0.1,\n",
    "                 dropout_embed:float=0.1,\n",
    "                 num_of_classes:int=5):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = EncoderV1(len_gm=len_gm,\n",
    "                                 patch_size=patch_size,\n",
    "                                 hidden_size=hidden_size,\n",
    "                                 num_heads=num_heads,\n",
    "                                 num_layers=num_layers,\n",
    "                                 dropout_attn=dropout_attn,\n",
    "                                 dropout_mlp=dropout_mlp,\n",
    "                                 dropout_embed=dropout_embed)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = DecoderV1(len_gm=len_gm,\n",
    "                                 patch_size=patch_size,\n",
    "                                 hidden_size=hidden_size,\n",
    "                                 num_heads=num_heads,\n",
    "                                 num_layers=num_layers,\n",
    "                                 dropout_attn=dropout_attn,\n",
    "                                 dropout_mlp=dropout_mlp,\n",
    "                                 dropout_embed=dropout_embed)\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = ClassifierV1(hidden_size=hidden_size,\n",
    "                                       num_of_classes=num_of_classes)\n",
    "        \n",
    "        # Splicer\n",
    "        self.splicer = SplicerV1(hidden_size=hidden_size,\n",
    "                                 patch_size=patch_size,\n",
    "                                 len_gm=len_gm)\n",
    "        \n",
    "    def forward(self, input_sequence, target_sequence=None, key_padding_mask=None, attn_mask=None):\n",
    "        # Encoder output\n",
    "        encoder_output = self.encoder(input_sequence, key_padding_mask=key_padding_mask)\n",
    "\n",
    "        encoder_output_to_decoder = encoder_output[:,1:13,:]\n",
    "\n",
    "        # If target sequence is provided, we are in training mode, otherwise we are in inference mode\n",
    "        if target_sequence is not None:\n",
    "            # training mode\n",
    "            decoder_output = self.decoder(output_encoder=encoder_output_to_decoder, \n",
    "                                          target_sequence=target_sequence, \n",
    "                                          attn_mask=attn_mask,\n",
    "                                          need_weights=True)\n",
    "            # Splicer forward pass to generate the dynamic response\n",
    "            dynamic_response = self.splicer(decoder_output)\n",
    "        else:\n",
    "            # inference mode\n",
    "            decoder_output = self.decoder(output_encoder=encoder_output_to_decoder,\n",
    "                                          target_sequence=None,\n",
    "                                          attn_mask=attn_mask,\n",
    "                                          need_weights=True)\n",
    "            \n",
    "            dynamic_response = self.splicer(decoder_output)\n",
    "\n",
    "        # Classifier forward pass to determine the damage state\n",
    "        # damage_state is logits\n",
    "        damage_state = self.classifier(encoder_output)\n",
    "\n",
    "        return damage_state, dynamic_response\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "EncoderV1.forward() got an unexpected keyword argument 'len_gm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mi:\\BaiduSyncdisk\\Project\\SeismicTransformer\\SeT_3_Cookbook.ipynb Cell 69\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/i%3A/BaiduSyncdisk/Project/SeismicTransformer/SeT_3_Cookbook.ipynb#Y125sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m SeismicTransformerV3_instance \u001b[39m=\u001b[39m SeismicTransformerV3()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/i%3A/BaiduSyncdisk/Project/SeismicTransformer/SeT_3_Cookbook.ipynb#Y125sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m input_gm \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m64\u001b[39m, \u001b[39m3000\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/i%3A/BaiduSyncdisk/Project/SeismicTransformer/SeT_3_Cookbook.ipynb#Y125sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m input_floorResponse \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m64\u001b[39m, \u001b[39m3000\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;32mi:\\BaiduSyncdisk\\Project\\SeismicTransformer\\SeT_3_Cookbook.ipynb Cell 69\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/i%3A/BaiduSyncdisk/Project/SeismicTransformer/SeT_3_Cookbook.ipynb#Y125sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m     <a href='vscode-notebook-cell:/i%3A/BaiduSyncdisk/Project/SeismicTransformer/SeT_3_Cookbook.ipynb#Y125sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Encoder\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/i%3A/BaiduSyncdisk/Project/SeismicTransformer/SeT_3_Cookbook.ipynb#Y125sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39m=\u001b[39m EncoderV1(len_gm\u001b[39m=\u001b[39;49mlen_gm,\n\u001b[0;32m     <a href='vscode-notebook-cell:/i%3A/BaiduSyncdisk/Project/SeismicTransformer/SeT_3_Cookbook.ipynb#Y125sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m                          patch_size\u001b[39m=\u001b[39;49mpatch_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/i%3A/BaiduSyncdisk/Project/SeismicTransformer/SeT_3_Cookbook.ipynb#Y125sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m                          hidden_size\u001b[39m=\u001b[39;49mhidden_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/i%3A/BaiduSyncdisk/Project/SeismicTransformer/SeT_3_Cookbook.ipynb#Y125sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m                          num_heads\u001b[39m=\u001b[39;49mnum_heads,\n\u001b[0;32m     <a href='vscode-notebook-cell:/i%3A/BaiduSyncdisk/Project/SeismicTransformer/SeT_3_Cookbook.ipynb#Y125sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m                          num_layers\u001b[39m=\u001b[39;49mnum_layers,\n\u001b[0;32m     <a href='vscode-notebook-cell:/i%3A/BaiduSyncdisk/Project/SeismicTransformer/SeT_3_Cookbook.ipynb#Y125sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m                          dropout_attn\u001b[39m=\u001b[39;49mdropout_attn,\n\u001b[0;32m     <a href='vscode-notebook-cell:/i%3A/BaiduSyncdisk/Project/SeismicTransformer/SeT_3_Cookbook.ipynb#Y125sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m                          dropout_mlp\u001b[39m=\u001b[39;49mdropout_mlp,\n\u001b[0;32m     <a href='vscode-notebook-cell:/i%3A/BaiduSyncdisk/Project/SeismicTransformer/SeT_3_Cookbook.ipynb#Y125sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m                          dropout_embed\u001b[39m=\u001b[39;49mdropout_embed)\n\u001b[0;32m     <a href='vscode-notebook-cell:/i%3A/BaiduSyncdisk/Project/SeismicTransformer/SeT_3_Cookbook.ipynb#Y125sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Decoder\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/i%3A/BaiduSyncdisk/Project/SeismicTransformer/SeT_3_Cookbook.ipynb#Y125sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder \u001b[39m=\u001b[39m DecoderV1(len_gm\u001b[39m=\u001b[39mlen_gm,\n\u001b[0;32m     <a href='vscode-notebook-cell:/i%3A/BaiduSyncdisk/Project/SeismicTransformer/SeT_3_Cookbook.ipynb#Y125sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m                          patch_size\u001b[39m=\u001b[39mpatch_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/i%3A/BaiduSyncdisk/Project/SeismicTransformer/SeT_3_Cookbook.ipynb#Y125sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m                          hidden_size\u001b[39m=\u001b[39mhidden_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/i%3A/BaiduSyncdisk/Project/SeismicTransformer/SeT_3_Cookbook.ipynb#Y125sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m                          dropout_mlp\u001b[39m=\u001b[39mdropout_mlp,\n\u001b[0;32m     <a href='vscode-notebook-cell:/i%3A/BaiduSyncdisk/Project/SeismicTransformer/SeT_3_Cookbook.ipynb#Y125sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m                          dropout_embed\u001b[39m=\u001b[39mdropout_embed)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;31mTypeError\u001b[0m: EncoderV1.forward() got an unexpected keyword argument 'len_gm'"
     ]
    }
   ],
   "source": [
    "SeismicTransformerV3_instance = SeismicTransformerV3().to(device)\n",
    "\n",
    "input_gm = torch.rand(64, 3000, 1).to(device)\n",
    "input_floorResponse = torch.rand(64, 3000, 1).to(device)\n",
    "key_padding_mask = torch.zeros(64, 14, dtype=torch.bool).to(device)\n",
    "attn_mask = torch.triu(torch.ones(12, 12), diagonal=1).bool().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training mode\n",
    "damage_state, dynamic_response = SeismicTransformerV3_instance(input_sequence=input_gm, \n",
    "                                                            target_sequence=input_floorResponse, \n",
    "                                                            key_padding_mask=key_padding_mask, \n",
    "                                                            attn_mask=attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 14, 5]), torch.Size([64, 3000, 1]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "damage_state.shape, dynamic_response.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference mode\n",
    "with torch.inference_mode():\n",
    "    damage_state, dynamic_response = SeismicTransformerV3_instance(input_sequence=input_gm, \n",
    "                                                                target_sequence=None, \n",
    "                                                                key_padding_mask=key_padding_mask, \n",
    "                                                                attn_mask=attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 14, 5]), torch.Size([64, 3000, 1]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "damage_state.shape, dynamic_response.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "loss_fn_classification = CrossEntropyLoss()\n",
    "loss_fn_regression = MSELoss()\n",
    "\n",
    "optimizer = torch.optim.AdamW(SeismicTransformerV3_instance.parameters(), lr=1e-4, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Tuple\n",
    "\n",
    "global global_step, warmup_done\n",
    "\n",
    "def train_step_set3(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader, \n",
    "               loss_fn_classification: torch.nn.Module, \n",
    "               loss_fn_regression: torch.nn.Module,\n",
    "               loss_fn_weight_classification: float,\n",
    "               optimizer: torch.optim.Optimizer, \n",
    "               lr_scheduler_warmup: torch.optim.lr_scheduler.LambdaLR, \n",
    "               num_warmup_steps: int, \n",
    "               device: torch.device) -> Tuple[float, float]:\n",
    "    \n",
    "    global global_step, warmup_done\n",
    "\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Initialize the loss and classification accuracy\n",
    "    train_loss, train_acc_classification, train_mse_regression = 0.0, 0.0, 0.0\n",
    "\n",
    "    for _, (gm_sequence, label, floor_sequence, key_padding_mask, attn_mask) in enumerate(dataloader):\n",
    "\n",
    "        gm_sequence = gm_sequence.to(device)\n",
    "        label = label.to(device)\n",
    "        floor_sequence = floor_sequence.to(device)\n",
    "        key_padding_mask = key_padding_mask.to(device)\n",
    "        attn_mask = attn_mask.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        damage_state_pred, dynamic_response = model(encoder_input=gm_sequence,\n",
    "                                                    decoder_input=floor_sequence,\n",
    "                                                    key_padding_mask=key_padding_mask,\n",
    "                                                    attn_mask=attn_mask)\n",
    "\n",
    "        # Calculate classification and regression losses\n",
    "        loss_classification = loss_fn_classification(damage_state_pred, label)\n",
    "        loss_regression = loss_fn_regression(dynamic_response, floor_sequence)\n",
    "\n",
    "        # Check that weight is between 0 and 1\n",
    "        assert 0 <= loss_fn_weight_classification <= 1, \\\n",
    "            \"loss_fn_weight_classification should be between 0 and 1\"\n",
    "\n",
    "        # Combine losses\n",
    "        loss = loss_fn_weight_classification * loss_classification + \\\n",
    "               (1 - loss_fn_weight_classification) * loss_regression\n",
    "\n",
    "        # Accumulate loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Zero gradients, backward pass, and optimizer step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update learning rate scheduler\n",
    "        lr_scheduler_warmup.step()\n",
    "        global_step += 1\n",
    "\n",
    "        # Check for warmup completion\n",
    "        if not warmup_done and global_step >= num_warmup_steps:\n",
    "            print(f\"Warmup completed at step {global_step}\")\n",
    "            warmup_done = True\n",
    "\n",
    "        # Early stopping in case of NaN loss\n",
    "        if torch.isnan(loss):\n",
    "            print(\"Loss is nan, stopping training.\")\n",
    "            break\n",
    "\n",
    "        # Calculate and accumulate classification accuracy\n",
    "        y_pred_class = torch.argmax(torch.softmax(damage_state_pred, dim=1), dim=1)\n",
    "        train_acc_classification += (y_pred_class == label).sum().item() / label.size(0)\n",
    "\n",
    "        # Regression MSE\n",
    "        mse = torch.nn.functional.mse_loss(dynamic_response, floor_sequence, reduction='sum').item()\n",
    "        train_mse_regression += mse / floor_sequence.numel()\n",
    "\n",
    "    # Average the accumulated loss and classification accuracy over all batches\n",
    "    train_loss /= len(dataloader)\n",
    "    train_acc_classification /= len(dataloader)\n",
    "    train_mse_regression /= len(dataloader)\n",
    "\n",
    "    return train_loss, train_acc_classification, train_mse_regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step_set3(model: torch.nn.Module,\n",
    "                         dataloader: torch.utils.data.DataLoader,\n",
    "                         loss_fn_classification: torch.nn.Module,\n",
    "                         loss_fn_regression: torch.nn.Module,\n",
    "                         loss_fn_weight_classification: float,\n",
    "                         device: torch.device) -> Tuple[float, float, float]:\n",
    "    model.eval()\n",
    "    val_loss, val_acc_classification, val_mse_regression = 0.0, 0.0, 0.0\n",
    "\n",
    "    # inference mode\n",
    "    with torch.inference_mode():\n",
    "        for _, (gm_sequence, label, floor_sequence, key_padding_mask, attn_mask) in enumerate(dataloader):\n",
    "            # Move data to device\n",
    "            gm_sequence = gm_sequence.to(device)\n",
    "            label = label.to(device)\n",
    "            floor_sequence = floor_sequence.to(device)\n",
    "            key_padding_mask = key_padding_mask.to(device)\n",
    "            attn_mask = attn_mask.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            damage_state_pred, dynamic_response = model(encoder_input=gm_sequence,\n",
    "                                                        decoder_input=floor_sequence,\n",
    "                                                        key_padding_mask=key_padding_mask,\n",
    "                                                        attn_mask=attn_mask)\n",
    "\n",
    "            # Calculate classification and regression losses\n",
    "            loss_classification = loss_fn_classification(damage_state_pred, label)\n",
    "            loss_regression = loss_fn_regression(dynamic_response, floor_sequence)\n",
    "            loss = loss_fn_weight_classification * loss_classification + \\\n",
    "                   (1 - loss_fn_weight_classification) * loss_regression\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            # Calculate classification accuracy\n",
    "            y_pred_class = torch.argmax(torch.softmax(damage_state_pred, dim=1), dim=1)\n",
    "            val_acc_classification += (y_pred_class == label).sum().item() / label.size(0)\n",
    "\n",
    "            # Regression MSE\n",
    "            mse = torch.nn.functional.mse_loss(dynamic_response, floor_sequence, reduction='sum').item()\n",
    "            val_mse_regression += mse / floor_sequence.numel()\n",
    "\n",
    "    val_loss /= len(dataloader)\n",
    "    val_acc_classification /= len(dataloader)\n",
    "    val_mse_regression /= len(dataloader)\n",
    "\n",
    "    return val_loss, val_acc_classification, val_mse_regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Train()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from PythonScripts.utility import LogEpochDataV3\n",
    "from typing import Dict, List\n",
    "import tqdm\n",
    "\n",
    "def train_set3(model: torch.nn.Module,\n",
    "               train_loader: torch.utils.data.DataLoader,\n",
    "               val_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn_classification: torch.nn.Module,\n",
    "               loss_fn_regression: torch.nn.Module,\n",
    "               loss_fn_weight_classification: float,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               lr_scheduler_warmup: torch.optim.lr_scheduler.LambdaLR,\n",
    "               lr_scheduler_decay: torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "               num_warmup_steps: int,\n",
    "               num_epochs: int,\n",
    "               device: torch.device,\n",
    "               log_filename: str) -> Dict[str, List]:\n",
    "    \n",
    "    # Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "               \"train_acc\": [],\n",
    "               \"train_mse\": [],\n",
    "               \"validation_loss\": [],\n",
    "               \"validation_acc\": [],\n",
    "               \"validation_mse\": [],\n",
    "               \"is_nan\": []\n",
    "    }\n",
    "\n",
    "    # epoch\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # train step\n",
    "        train_loss, train_acc, train_mse = train_step_set3(model, train_loader,\n",
    "                                                           loss_fn_classification,\n",
    "                                                           loss_fn_regression,\n",
    "                                                           loss_fn_weight_classification,\n",
    "                                                           optimizer,\n",
    "                                                           lr_scheduler_warmup,\n",
    "                                                           num_warmup_steps,\n",
    "                                                           device)\n",
    "        \n",
    "        # if train loss = nan, break\n",
    "        if math.isnan(train_loss):\n",
    "            print(f\"Epoch {epoch}:Train loss is NaN. Stopping training.\")\n",
    "            results[\"is_nan\"].append(\"yes\")\n",
    "            break\n",
    "        \n",
    "        # validation step\n",
    "        val_loss, val_acc, val_mse = validation_step_set3(model, val_loader,\n",
    "                                                         loss_fn_classification,\n",
    "                                                         loss_fn_regression,\n",
    "                                                         loss_fn_weight_classification,\n",
    "                                                         device)\n",
    "        \n",
    "        # if validation loss = nan, break\n",
    "        if math.isnan(val_loss):\n",
    "            print(f\"Epoch {epoch}:Validation loss is NaN. Stopping training.\")\n",
    "            results[\"is_nan\"].append(\"yes\")\n",
    "            break\n",
    "\n",
    "\n",
    "        # put validation_loss to lr_scheduler\n",
    "        lr_scheduler_decay.step(val_loss)\n",
    "\n",
    "\n",
    "        # update log file(csv file), need to modify and re-import\n",
    "        LogEpochDataV3(epoch=epoch,\n",
    "                     train_loss=train_loss,\n",
    "                     train_acc=train_acc,\n",
    "                     train_mse=train_mse,\n",
    "                     validation_loss=val_loss,\n",
    "                     validation_acc=val_acc,\n",
    "                     validation_mse=val_mse,\n",
    "                     log_filename=log_filename)\n",
    "\n",
    "        print(\n",
    "          f\"Epoch: {epoch+1} | \"\n",
    "          f\"train_loss: {train_loss:.4f} | \"\n",
    "          f\"train_acc: {train_acc:.4f} | \"\n",
    "          f\"train_mse: {train_mse:.4f} | \"\n",
    "          f\"validation_loss: {val_loss:.4f} | \"\n",
    "          f\"validation_acc: {val_acc:.4f} | \"\n",
    "          f\"validation_mse: {val_mse:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"train_mse\"].append(train_mse)\n",
    "        results[\"validation_loss\"].append(val_loss)\n",
    "        results[\"validation_acc\"].append(val_acc)\n",
    "        results[\"validation_mse\"].append(val_mse)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `test()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, recall_score, mean_squared_error\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "def test_set3(model: torch.nn.Module,\n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              device: torch.device) -> Tuple[Dict[str, float], Tuple[np.ndarray, np.ndarray]]:\n",
    "    '''\n",
    "\n",
    "    for test:\n",
    "    Cat all the predictions and true values, then compare them all at once, not one by one.\n",
    "    As long as the length of the two arrays is the same and fix (3000 for example), this method can work.\n",
    "\n",
    "    '''\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Initialize the correct predictions count and all predictions for classification\n",
    "    correct_label_preds = 0\n",
    "    total_label_preds = 0\n",
    "\n",
    "    y_label_preds = []\n",
    "    y_label_trues = []\n",
    "\n",
    "    # Initialize lists for dynamic response predictions and true values\n",
    "    dynamic_response_preds = []\n",
    "    dynamic_response_trues = []\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        \n",
    "        for _, (gm_sequence, label, floor_sequence, key_padding_mask, attn_mask) in enumerate(dataloader):\n",
    "\n",
    "            gm_sequence = gm_sequence.to(device)\n",
    "            label = label.to(device)\n",
    "            floor_sequence = floor_sequence.to(device)\n",
    "            key_padding_mask = key_padding_mask.to(device)\n",
    "            attn_mask = attn_mask.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            damage_state_pred, dynamic_response = model(encoder_input=gm_sequence,\n",
    "                                                        decoder_input=floor_sequence,\n",
    "                                                        key_padding_mask=key_padding_mask,\n",
    "                                                        attn_mask=attn_mask)\n",
    "            \n",
    "            # Calculate the correct predictions count (classification accuracy)\n",
    "            _, predicted_labels = torch.max(damage_state_pred, dim=1)\n",
    "            correct_label_preds += (predicted_labels == label).sum().item()\n",
    "            total_label_preds += label.size(0)\n",
    "\n",
    "            y_label_preds.append(predicted_labels.cpu())\n",
    "            y_label_trues.append(label.cpu())\n",
    "\n",
    "            # Append dynamic response predictions and true values\n",
    "            dynamic_response_preds.append(dynamic_response.cpu())\n",
    "            dynamic_response_trues.append(floor_sequence.cpu())\n",
    "    \n",
    "    # Convert predictions and actual labels from list to single tensor\n",
    "    y_label_preds_tensor = torch.cat(y_label_preds)\n",
    "    y_label_trues_tensor = torch.cat(y_label_trues)\n",
    "\n",
    "    # convert to numpy array for classification metrics\n",
    "    y_label_preds_numpy = y_label_preds_tensor.numpy()\n",
    "    y_label_trues_numpy = y_label_trues_tensor.numpy()\n",
    "\n",
    "    # Calculate overall accuracy for classification\n",
    "    test_acc = correct_label_preds / total_label_preds if total_label_preds > 0 else 0.0\n",
    "\n",
    "    # Calculate F1 score for classification\n",
    "    test_f1 = f1_score(y_label_trues_numpy, y_label_preds_numpy, average='macro')\n",
    "\n",
    "    # Calculate Recall score for classification\n",
    "    test_Recall = recall_score(y_label_trues_numpy, y_label_preds_numpy, average='macro')\n",
    "\n",
    "    # Convert dynamic response predictions and actual values from list to single tensor\n",
    "    dynamic_response_preds_tensor = torch.cat(dynamic_response_preds)\n",
    "    dynamic_response_trues_tensor = torch.cat(dynamic_response_trues)\n",
    "\n",
    "    # Convert to numpy array for MSE calculation\n",
    "    dynamic_response_preds_numpy = dynamic_response_preds_tensor.numpy()\n",
    "    dynamic_response_trues_numpy = dynamic_response_trues_tensor.numpy()\n",
    "\n",
    "    # Calculate MSE for dynamic response predictions\n",
    "    test_mse = mean_squared_error(dynamic_response_trues_numpy, dynamic_response_preds_numpy)\n",
    "\n",
    "    # Build the results dictionary\n",
    "    results = {\n",
    "        'test_accuracy': test_acc,\n",
    "        'f1_score': test_f1,\n",
    "        'recall_score': test_Recall,\n",
    "        'mse_dynamic_response': test_mse,\n",
    "    }\n",
    "\n",
    "    # Return results and prediction values\n",
    "    return results, (y_label_preds_numpy, y_label_trues_numpy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
